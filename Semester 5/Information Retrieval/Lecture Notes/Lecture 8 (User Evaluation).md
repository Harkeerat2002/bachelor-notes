#lectureNote
# IR evaluation
- Types of Evaluation
	- **System Evaluation**
	- **User Evaluation**
	- **Operational Evaluation**
## System Evaluation
- Test Collection 
- High volume of queries
- Test quality of IR system
- No user involvement
- Cheap and popular
## User Evaluation
- Tests quality of IR system + user interaction (functionalities for document display, query reformulation, relevance feedback, etc)
- Low volume of queries
	- Usually, because of the cost of user time
- Direct user involvement, but artificial tasks
## Operational Evaluation
- Similar to user evaluation but with real users in real search situations and with real tasks on a real collection
- Expensive and difficult to run, but very good test of the system in a real situation

# System Evaluation
- Involves, typically, a test collection consists of
	- Collection of documents
	- Queries
	- Relevance Judgments
- **Good Points:**
	- Cheap
	- Allow for comparison
- **Bad Points:**
	- It only evaluates the engine part of the system

# User-Oriented Evaluation
- Evaluate the whole system
- **User-Oriented Evaluation features:**
	- Still comparative
	- Use objective measures
	- But also use subjective measures
	- Qualitative and quantitative analysis
- The environment is less controlled
	- **Main Idea:** User Evaluate the System
	- But users have:
		- Different **knowledge/Abilities/Experience**
- **Things that we can control:**
	- Tasks
	- Time taken to search
	- Instructions given
	- Training/help
- **Things we cannot control:**
	- User background knowledge
	- User's mood on the day of the experiment
	- Motivations

# Operational Evaluation
- The most complete type of evaluation
- No control over searching
	- Searchers bring own tasks and document collection
	- Searchers decide when to quit
	- Usually no training for users
- It needs a very careful design of the few variables under our own control.